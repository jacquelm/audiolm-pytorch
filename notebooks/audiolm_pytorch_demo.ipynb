{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n337KoD2om3L",
    "outputId": "97ada0c6-f21c-483e-d63d-08abddd49004"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 28 13:51:13 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.161.07             Driver Version: 535.161.07   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1650        Off | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   53C    P8               4W /  60W |     59MiB /  4096MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      3268      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    0   N/A  N/A      3585    C+G   ...libexec/gnome-remote-desktop-daemon       51MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "# If this doesn't work, there's no GPU available or detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TLJAcUHpvmp4",
    "outputId": "95bcda95-a484-40c6-e5a7-47f4378759a8"
   },
   "outputs": [],
   "source": [
    "# !pip install audiolm-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xuNcsDJsvQwh"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Includes:\n",
    "\n",
    "- How to generate a placeholder dataset if you haven't already, just the basics to run \"training\" e2e on a tiny dataset\n",
    "- How to download a dataset from OpenSLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBxNK5cKW--_"
   },
   "source": [
    "### Imports & paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OrNeKngVVM0L"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxime/miniforge3/envs/audiolm/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "mutable default <class 'fairseq.dataclass.configs.CommonConfig'> for field common is not allowed: use default_factory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtarfile\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maudiolm_pytorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SoundStream, SoundStreamTrainer, HubertWithKmeans, SemanticTransformer, SemanticTransformerTrainer, HubertWithKmeans, CoarseTransformer, CoarseTransformerWrapper, CoarseTransformerTrainer, FineTransformer, FineTransformerWrapper, FineTransformerTrainer, AudioLM\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Code/Neural_Network/Pytorch/audiolm-pytorch/audiolm_pytorch/__init__.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meinops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_torch_specific\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m allow_ops_in_compiled_graph\n\u001b[1;32m      6\u001b[0m     allow_ops_in_compiled_graph()\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maudiolm_pytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudiolm_pytorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AudioLM\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maudiolm_pytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msoundstream\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SoundStream, AudioLMSoundStream, MusicLMSoundStream\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maudiolm_pytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mencodec\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EncodecWrapper\n",
      "File \u001b[0;32m~/Documents/Code/Neural_Network/Pytorch/audiolm-pytorch/audiolm_pytorch/audiolm_pytorch.py:18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meinops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rearrange, repeat, reduce\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meinops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Rearrange\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maudiolm_pytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvq_wav2vec\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FairseqVQWav2Vec\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maudiolm_pytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhubert_kmeans\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HubertWithKmeans\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maudiolm_pytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mt5\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m t5_encode_text, get_encoded_dim, DEFAULT_T5_NAME\n",
      "File \u001b[0;32m~/Documents/Code/Neural_Network/Pytorch/audiolm-pytorch/audiolm_pytorch/vq_wav2vec.py:7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meinops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rearrange\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfairseq\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchaudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m resample\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maudiolm_pytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m curtail_to_multiple\n",
      "File \u001b[0;32m~/miniforge3/envs/audiolm/lib/python3.12/site-packages/fairseq/__init__.py:20\u001b[0m\n\u001b[1;32m     17\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpdb\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# backwards compatibility to support `from fairseq.X import Y`\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfairseq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils \u001b[38;5;28;01mas\u001b[39;00m distributed_utils\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfairseq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m meters, metrics, progress_bar  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m     23\u001b[0m sys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfairseq.distributed_utils\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m distributed_utils\n",
      "File \u001b[0;32m~/miniforge3/envs/audiolm/lib/python3.12/site-packages/fairseq/distributed/__init__.py:7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Facebook, Inc. and its affiliates.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# This source code is licensed under the MIT license found in the\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed_timeout_wrapper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DistributedTimeoutWrapper\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfully_sharded_data_parallel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      8\u001b[0m     fsdp_enable_wrap,\n\u001b[1;32m      9\u001b[0m     fsdp_wrap,\n\u001b[1;32m     10\u001b[0m     FullyShardedDataParallel,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy_distributed_data_parallel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LegacyDistributedDataParallel\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodule_proxy_wrapper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModuleProxyWrapper\n",
      "File \u001b[0;32m~/miniforge3/envs/audiolm/lib/python3.12/site-packages/fairseq/distributed/fully_sharded_data_parallel.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optional\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfairseq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataclass\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfigs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DistributedTrainingConfig\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfairseq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils \u001b[38;5;28;01mas\u001b[39;00m dist_utils\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/audiolm/lib/python3.12/site-packages/fairseq/dataclass/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Facebook, Inc. and its affiliates.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# This source code is licensed under the MIT license found in the\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfigs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FairseqDataclass\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChoiceEnum\n\u001b[1;32m     10\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFairseqDataclass\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChoiceEnum\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m ]\n",
      "File \u001b[0;32m~/miniforge3/envs/audiolm/lib/python3.12/site-packages/fairseq/dataclass/configs.py:1104\u001b[0m\n\u001b[1;32m   1095\u001b[0m     ema_update_freq: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m field(\n\u001b[1;32m   1096\u001b[0m         default\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, metadata\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhelp\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDo EMA update every this many model updates\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m   1097\u001b[0m     )\n\u001b[1;32m   1098\u001b[0m     ema_fp32: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m field(\n\u001b[1;32m   1099\u001b[0m         default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1100\u001b[0m         metadata\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhelp\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf true, store EMA model in fp32 even if model is in fp16\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m   1101\u001b[0m     )\n\u001b[0;32m-> 1104\u001b[0m \u001b[38;5;129;43m@dataclass\u001b[39;49m\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mFairseqConfig\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mFairseqDataclass\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommon\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mCommonConfig\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mCommonConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommon_eval\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mCommonEvalConfig\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mCommonEvalConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/audiolm/lib/python3.12/dataclasses.py:1266\u001b[0m, in \u001b[0;36mdataclass\u001b[0;34m(cls, init, repr, eq, order, unsafe_hash, frozen, match_args, kw_only, slots, weakref_slot)\u001b[0m\n\u001b[1;32m   1263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wrap\n\u001b[1;32m   1265\u001b[0m \u001b[38;5;66;03m# We're called as @dataclass without parens.\u001b[39;00m\n\u001b[0;32m-> 1266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/audiolm/lib/python3.12/dataclasses.py:1256\u001b[0m, in \u001b[0;36mdataclass.<locals>.wrap\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m   1255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap\u001b[39m(\u001b[38;5;28mcls\u001b[39m):\n\u001b[0;32m-> 1256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_process_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrepr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munsafe_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1257\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mfrozen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatch_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkw_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslots\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1258\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mweakref_slot\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/audiolm/lib/python3.12/dataclasses.py:994\u001b[0m, in \u001b[0;36m_process_class\u001b[0;34m(cls, init, repr, eq, order, unsafe_hash, frozen, match_args, kw_only, slots, weakref_slot)\u001b[0m\n\u001b[1;32m    991\u001b[0m         kw_only \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    992\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    993\u001b[0m         \u001b[38;5;66;03m# Otherwise it's a field of some type.\u001b[39;00m\n\u001b[0;32m--> 994\u001b[0m         cls_fields\u001b[38;5;241m.\u001b[39mappend(\u001b[43m_get_field\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkw_only\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    996\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m cls_fields:\n\u001b[1;32m    997\u001b[0m     fields[f\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m f\n",
      "File \u001b[0;32m~/miniforge3/envs/audiolm/lib/python3.12/dataclasses.py:852\u001b[0m, in \u001b[0;36m_get_field\u001b[0;34m(cls, a_name, a_type, default_kw_only)\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[38;5;66;03m# For real fields, disallow mutable defaults.  Use unhashable as a proxy\u001b[39;00m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;66;03m# indicator for mutability.  Read the __hash__ attribute from the class,\u001b[39;00m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;66;03m# not the instance.\u001b[39;00m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39m_field_type \u001b[38;5;129;01mis\u001b[39;00m _FIELD \u001b[38;5;129;01mand\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdefault\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__hash__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 852\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmutable default \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(f\u001b[38;5;241m.\u001b[39mdefault)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for field \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    853\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not allowed: use default_factory\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f\n",
      "\u001b[0;31mValueError\u001b[0m: mutable default <class 'fairseq.dataclass.configs.CommonConfig'> for field common is not allowed: use default_factory"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import math\n",
    "import wave\n",
    "import struct\n",
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "from audiolm_pytorch import SoundStream, SoundStreamTrainer, HubertWithKmeans, SemanticTransformer, SemanticTransformerTrainer, HubertWithKmeans, CoarseTransformer, CoarseTransformerWrapper, CoarseTransformerTrainer, FineTransformer, FineTransformerWrapper, FineTransformerTrainer, AudioLM\n",
    "from torch import nn\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "\n",
    "# define all dataset paths, checkpoints, etc\n",
    "dataset_folder = \"placeholder_dataset\"\n",
    "soundstream_ckpt = \"results/soundstream.8.pt\" # this can change depending on number of steps\n",
    "hubert_ckpt = 'hubert/hubert_base_ls960.pt'\n",
    "hubert_quantizer = f'hubert/hubert_base_ls960_L9_km500.bin' # listed in row \"HuBERT Base (~95M params)\", column Quantizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pA56YODZXBtf"
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "6nnPceFWwedh"
   },
   "outputs": [],
   "source": [
    "# Placeholder data generation\n",
    "def get_sinewave(freq=440.0, duration_ms=200, volume=1.0, sample_rate=44100.0):\n",
    "  # code adapted from https://stackoverflow.com/a/33913403\n",
    "  audio = []\n",
    "  num_samples = duration_ms * (sample_rate / 1000.0)\n",
    "  for x in range(int(num_samples)):\n",
    "    audio.append(volume * math.sin(2 * math.pi * freq * (x / sample_rate)))\n",
    "  return audio\n",
    "\n",
    "def save_wav(file_name, audio, sample_rate=44100.0):\n",
    "  # Open up a wav file\n",
    "  wav_file=wave.open(file_name,\"w\")\n",
    "  # wav params\n",
    "  nchannels = 1\n",
    "  sampwidth = 2\n",
    "  # 44100 is the industry standard sample rate - CD quality.  If you need to\n",
    "  # save on file size you can adjust it downwards. The stanard for low quality\n",
    "  # is 8000 or 8kHz.\n",
    "  nframes = len(audio)\n",
    "  comptype = \"NONE\"\n",
    "  compname = \"not compressed\"\n",
    "  wav_file.setparams((nchannels, sampwidth, sample_rate, nframes, comptype, compname))\n",
    "  # WAV files here are using short, 16 bit, signed integers for the \n",
    "  # sample size.  So we multiply the floating point data we have by 32767, the\n",
    "  # maximum value for a short integer.  NOTE: It is theortically possible to\n",
    "  # use the floating point -1.0 to 1.0 data directly in a WAV file but not\n",
    "  # obvious how to do that using the wave module in python.\n",
    "  for sample in audio:\n",
    "      wav_file.writeframes(struct.pack('h', int( sample * 32767.0 )))\n",
    "  wav_file.close()\n",
    "  return\n",
    "\n",
    "def make_placeholder_dataset():\n",
    "  # Make a placeholder dataset with a few .wav files that you can \"train\" on, just to verify things work e2e\n",
    "  if os.path.isdir(dataset_folder):\n",
    "    return\n",
    "  os.makedirs(dataset_folder)\n",
    "  save_wav(f\"{dataset_folder}/example.wav\", get_sinewave())\n",
    "  save_wav(f\"{dataset_folder}/example2.wav\", get_sinewave(duration_ms=500))\n",
    "  os.makedirs(f\"{dataset_folder}/subdirectory\")\n",
    "  save_wav(f\"{dataset_folder}/subdirectory/example.wav\", get_sinewave(freq=330.0))\n",
    "\n",
    "make_placeholder_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "jwYCbFpHvmRI"
   },
   "outputs": [],
   "source": [
    "# Get actual dataset. Uncomment this if you want to try training on real data\n",
    "\n",
    "# full dataset: https://www.openslr.org/12\n",
    "# We'll use https://us.openslr.org/resources/12/dev-clean.tar.gz development set, \"clean\" speech.\n",
    "# We *should* train on, well, training, but this is just to demo running things end-to-end at all so I just picked a small clean set.\n",
    "\n",
    "# url = \"https://us.openslr.org/resources/12/dev-clean.tar.gz\"\n",
    "# filename = \"dev-clean\"\n",
    "# filename_targz = filename + \".tar.gz\"\n",
    "# if not os.path.isfile(filename_targz):\n",
    "#   urllib.request.urlretrieve(url, filename_targz)\n",
    "# if not os.path.isdir(filename):\n",
    "#   # open file\n",
    "#   with tarfile.open(filename_targz) as t:\n",
    "#     t.extractall(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PYcI0aXEwuxR"
   },
   "source": [
    "## Training\n",
    "\n",
    "Now that we have a dataset, we can train AudioLM.\n",
    "\n",
    "**Note**: do NOT type \"y\" to overwrite previous experiments/ checkpoints when running through the cells here unless you're ready to the entire results folder! Otherwise you will end up erasing things (e.g. you train SoundStream first, and if you choose \"overwrite\" then you lose the SoundStream checkpoint when you then train SemanticTransformer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7GiyBcBWiZV"
   },
   "source": [
    "### SoundStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nGU0OZiOwPEO",
    "outputId": "21dd959c-6458-4477-8403-cf810166f38d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with dataset of 2 samples and validating with randomly splitted 1 samples\n",
      "0: soundstream total loss: 167.262, soundstream recon loss: 1.123 | discr (scale 1) loss: 2.003 | discr (scale 0.5) loss: 1.999 | discr (scale 0.25) loss: 1.999\n",
      "0: saving to results\n",
      "0: saving model to results\n",
      "1: soundstream total loss: 182.282, soundstream recon loss: 1.389 | discr (scale 1) loss: 1.938 | discr (scale 0.5) loss: 1.928 | discr (scale 0.25) loss: 1.928\n",
      "2: soundstream total loss: 196.668, soundstream recon loss: 1.450 | discr (scale 1) loss: 1.845 | discr (scale 0.5) loss: 1.842 | discr (scale 0.25) loss: 1.843\n",
      "2: saving to results\n",
      "3: soundstream total loss: 216.329, soundstream recon loss: 1.451 | discr (scale 1) loss: 1.751 | discr (scale 0.5) loss: 1.750 | discr (scale 0.25) loss: 1.757\n",
      "4: soundstream total loss: 206.804, soundstream recon loss: 1.167 | discr (scale 1) loss: 1.671 | discr (scale 0.5) loss: 1.706 | discr (scale 0.25) loss: 1.724\n",
      "4: saving to results\n",
      "4: saving model to results\n",
      "5: soundstream total loss: 195.325, soundstream recon loss: 0.929 | discr (scale 1) loss: 1.348 | discr (scale 0.5) loss: 1.372 | discr (scale 0.25) loss: 1.482\n",
      "6: soundstream total loss: 245.195, soundstream recon loss: 1.054 | discr (scale 1) loss: 1.060 | discr (scale 0.5) loss: 1.244 | discr (scale 0.25) loss: 1.288\n",
      "6: saving to results\n",
      "7: soundstream total loss: 245.724, soundstream recon loss: 0.970 | discr (scale 1) loss: 1.092 | discr (scale 0.5) loss: 1.358 | discr (scale 0.25) loss: 1.079\n",
      "8: soundstream total loss: 202.707, soundstream recon loss: 0.786 | discr (scale 1) loss: 0.733 | discr (scale 0.5) loss: 0.687 | discr (scale 0.25) loss: 0.790\n",
      "8: saving to results\n",
      "8: saving model to results\n",
      "training complete\n"
     ]
    }
   ],
   "source": [
    "soundstream = SoundStream(\n",
    "    codebook_size = 1024,\n",
    "    rq_num_quantizers = 8,\n",
    ")\n",
    "\n",
    "trainer = SoundStreamTrainer(\n",
    "    soundstream,\n",
    "    folder = dataset_folder,\n",
    "    batch_size = 4,\n",
    "    grad_accum_every = 8,         # effective batch size of 32\n",
    "    data_max_length = 320 * 32,\n",
    "    save_results_every = 2,\n",
    "    save_model_every = 4,\n",
    "    num_train_steps = 9\n",
    ").cuda()\n",
    "# NOTE: I changed num_train_steps to 9 (aka 8 + 1) from 10000 to make things go faster for demo purposes\n",
    "# adjusting save_*_every variables for the same reason\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqjN28L4Wc5Q"
   },
   "source": [
    "### SemanticTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qgd962eSvDzS",
    "outputId": "b0550cde-0c8b-4a39-f896-f6f813f50f8c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator MiniBatchKMeans from version 0.24.0 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with dataset of 2 samples and validating with randomly splitted 1 samples\n",
      "do you want to clear previous experiment checkpoints and results? (y/n) n\n",
      "0: loss: 6.648584365844727\n",
      "0: valid loss 5.763116359710693\n",
      "0: saving model to results\n",
      "training complete\n"
     ]
    }
   ],
   "source": [
    "# hubert checkpoints can be downloaded at\n",
    "# https://github.com/facebookresearch/fairseq/tree/main/examples/hubert\n",
    "if not os.path.isdir(\"hubert\"):\n",
    "  os.makedirs(\"hubert\")\n",
    "if not os.path.isfile(hubert_ckpt):\n",
    "  hubert_ckpt_download = f\"https://dl.fbaipublicfiles.com/{hubert_ckpt}\"\n",
    "  urllib.request.urlretrieve(hubert_ckpt_download, f\"./{hubert_ckpt}\")\n",
    "if not os.path.isfile(hubert_quantizer):\n",
    "  hubert_quantizer_download = f\"https://dl.fbaipublicfiles.com/{hubert_quantizer}\"\n",
    "  urllib.request.urlretrieve(hubert_quantizer_download, f\"./{hubert_quantizer}\")\n",
    "\n",
    "wav2vec = HubertWithKmeans(\n",
    "    checkpoint_path = f'./{hubert_ckpt}',\n",
    "    kmeans_path = f'./{hubert_quantizer}'\n",
    ")\n",
    "\n",
    "semantic_transformer = SemanticTransformer(\n",
    "    num_semantic_tokens = wav2vec.codebook_size,\n",
    "    dim = 1024,\n",
    "    depth = 6\n",
    ").cuda()\n",
    "\n",
    "\n",
    "trainer = SemanticTransformerTrainer(\n",
    "    transformer = semantic_transformer,\n",
    "    wav2vec = wav2vec,\n",
    "    folder = dataset_folder,\n",
    "    batch_size = 1,\n",
    "    data_max_length = 320 * 32,\n",
    "    num_train_steps = 1\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4eEvIzhEWwRz"
   },
   "source": [
    "### CoarseTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1LeWmaNHzzY9",
    "outputId": "7e7ecb3b-f59e-4d18-c8c9-64762e9b43fc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator MiniBatchKMeans from version 0.24.0 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with dataset of 2 samples and validating with randomly splitted 1 samples\n",
      "do you want to clear previous experiment checkpoints and results? (y/n) n\n",
      "0: loss: 63.983970642089844\n",
      "0: valid loss 63.398582458496094\n",
      "0: saving model to results\n",
      "1: loss: 65.85967254638672\n",
      "2: loss: 62.4722900390625\n",
      "2: valid loss 50.01605987548828\n",
      "3: loss: 11.735434532165527\n",
      "4: loss: 3.976104497909546\n",
      "4: valid loss 46.094608306884766\n",
      "4: saving model to results\n",
      "5: loss: 58.27140426635742\n",
      "6: loss: 41.68347930908203\n",
      "6: valid loss 45.54595184326172\n",
      "7: loss: 2.2387890815734863\n",
      "8: loss: 0.4718627631664276\n",
      "8: valid loss 39.10848617553711\n",
      "8: saving model to results\n",
      "training complete\n"
     ]
    }
   ],
   "source": [
    "wav2vec = HubertWithKmeans(\n",
    "    checkpoint_path = f'./{hubert_ckpt}',\n",
    "    kmeans_path = f'./{hubert_quantizer}'\n",
    ")\n",
    "\n",
    "soundstream = SoundStream(\n",
    "    codebook_size = 1024,\n",
    "    rq_num_quantizers = 8,\n",
    ")\n",
    "\n",
    "soundstream.load(f\"./{soundstream_ckpt}\")\n",
    "\n",
    "coarse_transformer = CoarseTransformer(\n",
    "    num_semantic_tokens = wav2vec.codebook_size,\n",
    "    codebook_size = 1024,\n",
    "    num_coarse_quantizers = 3,\n",
    "    dim = 512,\n",
    "    depth = 6\n",
    ")\n",
    "\n",
    "trainer = CoarseTransformerTrainer(\n",
    "    transformer = coarse_transformer,\n",
    "    codec = soundstream,\n",
    "    wav2vec = wav2vec,\n",
    "    folder = dataset_folder,\n",
    "    batch_size = 1,\n",
    "    data_max_length = 320 * 32,\n",
    "    save_results_every = 2,\n",
    "    save_model_every = 4,\n",
    "    num_train_steps = 9\n",
    ")\n",
    "# NOTE: I changed num_train_steps to 9 (aka 8 + 1) from 10000 to make things go faster for demo purposes\n",
    "# adjusting save_*_every variables for the same reason\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fRvj7qOJWzmw"
   },
   "source": [
    "### FineTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZRaEhRRKWg8F",
    "outputId": "7cc166c4-c8e9-45ef-8293-8f5381c2d3af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with dataset of 2 samples and validating with randomly splitted 1 samples\n",
      "do you want to clear previous experiment checkpoints and results? (y/n) n\n",
      "0: loss: 70.90608215332031\n",
      "0: valid loss 65.99951171875\n",
      "0: saving model to results\n",
      "1: loss: 43.6014289855957\n",
      "2: loss: 8.300681114196777\n",
      "3: loss: 61.23375701904297\n",
      "4: loss: 63.34052276611328\n",
      "5: loss: 2.010118246078491\n",
      "6: loss: 56.52588653564453\n",
      "7: loss: 0.5423888564109802\n",
      "8: loss: 0.005095238331705332\n",
      "training complete\n"
     ]
    }
   ],
   "source": [
    "soundstream = SoundStream(\n",
    "    codebook_size = 1024,\n",
    "    rq_num_quantizers = 8,\n",
    ")\n",
    "\n",
    "soundstream.load(f\"./{soundstream_ckpt}\")\n",
    "\n",
    "fine_transformer = FineTransformer(\n",
    "    num_coarse_quantizers = 3,\n",
    "    num_fine_quantizers = 5,\n",
    "    codebook_size = 1024,\n",
    "    dim = 512,\n",
    "    depth = 6\n",
    ")\n",
    "\n",
    "trainer = FineTransformerTrainer(\n",
    "    transformer = fine_transformer,\n",
    "    codec = soundstream,\n",
    "    folder = dataset_folder,\n",
    "    batch_size = 1,\n",
    "    data_max_length = 320 * 32,\n",
    "    num_train_steps = 9\n",
    ")\n",
    "# NOTE: I changed num_train_steps to 9 (aka 8 + 1) from 10000 to make things go faster for demo purposes\n",
    "# adjusting save_*_every variables for the same reason\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QoHgkgA3XKXH"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rzghrux5WinW",
    "outputId": "9dd39f7f-0046-4a5f-826e-a442345987af"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating semantic:   0%|          | 10/2048 [00:00<00:25, 78.55it/s]\n",
      "generating coarse: 100%|██████████| 512/512 [00:14<00:00, 34.83it/s]\n",
      "generating fine: 100%|██████████| 512/512 [02:56<00:00,  2.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# Everything together\n",
    "audiolm = AudioLM(\n",
    "    wav2vec = wav2vec,\n",
    "    codec = soundstream,\n",
    "    semantic_transformer = semantic_transformer,\n",
    "    coarse_transformer = coarse_transformer,\n",
    "    fine_transformer = fine_transformer\n",
    ")\n",
    "\n",
    "generated_wav = audiolm(batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "4rQPHTSRngEr"
   },
   "outputs": [],
   "source": [
    "output_path = \"out.wav\"\n",
    "sample_rate = 44100\n",
    "torchaudio.save(output_path, generated_wav.cpu(), sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "is9wLY_ncDYK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
